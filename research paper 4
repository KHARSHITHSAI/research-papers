Enhancing the F1 Score of Detecting the Emotion from the Text using Novel Naïve Bayes Algorithm in Comparison with Bidirectional Encoder Representations from Transformers Algorithm and Support Vector Machine  Algorithm
                                       	K. Harshith Sai1, D. Beulah David2
 
K. Harshith Sai1
Research Scholar,
Department of Computer Science and Engineering,
Saveetha School of Engineering,
Saveetha Institute of Medical and Technical Sciences,
Saveetha University, Chennai, TamilNadu, India, Pincode: 602105.
harshithsaik1303.sse@saveetha.com
 
D. Beulah David2
Project Guide, Corresponding Author,
Department of Quantum Intelligence,
Saveetha School of Engineering,
Saveetha Institute of Medical and Technical Sciences, VV
Saveetha University, Chennai, TamilNadu, India, Pincode: 602105.
beulahdavid.sse@saveetha.com

Keywords: Novel Naïve Bayes, Bidirectional Encoder Representations from Transformers, Support Vector fMachine , Emotion, F1 Score, Machine learning.
ABSTRACT
Aim: To find the F1 Score in detecting the emotion from the text using Novel Naïve Bayes Algorithm and Bidirectional Encoder Representations from Transformers Algorithm and Support Vector Machine  Algorithm. Materials and Methods: Novel Naive Bahhgugyes (N=20) and Bidirectional Encoder Representations from Transformers (N=20) and Support Vector Machine  (N=20) are applied to the datasets of detecting the emotion from the text which includes a total of 5938 rows and 2 columns for enhancing the F1 Score in duetecting the emotion from the text. The sample size of 20 in each group  is estimated using Clincalc with the G power of 0.80 and the alpha α=0.05. Depending on the F1 Score, the detection of the emotion from the text is evaluated. Results: Novel Naïve Bayes (93.925%) is more accurate in detecting the emotion from the text than Bidirectional Encoder Representations from Transformers (50%) and Support Vector Machine  (89.61%). This indicates that there is a statistically significant difference between the three algorithms (One-way Anova) Conclusion: The mean F1 Score of Novel Naïve Bayes is significantly better when compared to Bidirectional Encoder Representations from Transformers and Support Vector Machine . The Mean F1 Score of the Novel Naive Bayes is higher than Bidirectional Encoder Representations from Transformers and Support Vector Machine .
 
Keywords: Novel Naïve Bayes, Bidirectional Encoder Representations from Transformers, Support Vector Machine , Emotion, F1 Score, Machine Learning.
INTRODUCTION
        	It is becoming more and more important to comprehend and interpret human emotions from textual data in the modern world of communication and technology. This work explores the field of emotion detection from text with the goal of creating strong models that are able to recognize and interpret the finer points of human emotions as they are expressed in written language. The research holds importance as it has the potential to improve  domains, including human-computer interaction, sentiment analysis, mental health monitoring, and customer service. The ability to recognize and  to emotions in text offers great potential for enhancing human relationships and experiences as we navigate a world where digital communication predominates.
        	In the realm of database research, numerous scholars have contributed valuable insights to advance emotion analysis. Among the most relevant papers,  Emotion Detection From Tweets Using a BERT and SVM Ensemble Model by Lonut-Alexandru Albu,Stelian Spinu (2022), Recognizing Emotions from Texts using a Bert-Based Approach by Acheampong Francisca Adoma; Nunoo-Mensah Henry; Wenyu Chen; Niyongabo Rubungo Andre (2022),Chikkili.Hema Kumar,R.Senthil Kumar (2020), Notably, Comparison of SVM and Naive Bayes for sentiment classification using BERT data by Shivani Rana; Rakesh Kanji; Shruti Jain (2022) remains a cornerstone in the field. This study not only provides a benchmark for evaluating different algorithms but also offers valuable insights into the challenges and opportunities inherent in emotion analysis within the dynamic context of social media.
        	Despite the advancements in emotion analysis, a notable research gap persists in understanding the nuanced intricacies of emotions expressed in text, especially in social media settings. This research seeks to address this gap by leveraging expertise in natural language processing and machine learning. In a significant contribution to this field,Comparison of SVM and Naive Bayes for sentiment classification using BERT data by Shivani Rana; Rakesh Kanji; Shruti Jain (2022). Their work stands out as a pivotal study in the realm of emotion detection from text, particularly in the dynamic and diverse landscape of social media discourse enhancing their applicability and reliability in real-world scenarios.

.
MATERIALS AND METHODS
	This research study was conducted in the Quantum Intelligence Laboratory of the Computer Science Engineering Department at the Saveetha School of Engineering, Saveetha Institute of Medical and Technical Sciences. This research work consists of two sample groups. Each group consists of sample size 20 in total (N=20). Novel Naive Bayes and Bidirectional Encoder Representations from Transformers and Support Vector Machine  were the two algorithms in Machine learning that were used to compare the datasets.
The datasets are taken from https://www.kaggle.com/datasets/abdallahwagih/emotion-dataset which was stored in .csv format.The file consists of 5938 rows and 2 columns.For the Novel Naive Bayes, 25% of the whole dataset was used as the test size and the remaining 75% was used as the training set. The whole dataset was fitted for training the Novel Naive Bayes and Bidirectional Encoder Representations from Transformers and Support Vector Machine  in Machine learning. Using Python 3.11, the F1 Score of both the models was y on a sample size of 20. 
Novel Naive Bayes
    The Naive Bayes algorithm, a probabilistic classification method rooted in Bayes' theorem, proves invaluable in the realm of emotion detection from textual data. When applied to categorize me u text into predefined emotion categories such as happy, sad, or angry, Naive Bayes calculates the probability of a hypothesis based on observed evidence. This evidence comprises the words or features present in the text, and the algorithm leverages the independence assumption – a "naive" belief that the presence of each word is unrelated to the presence of others, simplifying the calculation.
Detecting emotions from text using the Naive Bayes algorithm involves using probability theory to classify text into different emotional categories. Here's the formula and explanation for this process:
Formula:
Given a text ( X ), we want to classify it into one of the predefined emotion categories ( E1, E2, ..., En).
We calculate the probability of each emotion given the text using Bayes' Theorem:
 P(Ei | X) = (P(X | Ei) . P(Ei))/P(X)

Where:
- ( P(Ei | X) ) is the probability of emotion ( Ei ) given text ( X ).
- ( P(X | Ei) ) is the probability of text ( X ) given emotion ( Ei ) (likelihood).
- ( P(Ei) ) is the prior probability of emotion ( Ei ).
- ( P(X) ) is the probability of observing text ( X ).
Since ( P(X) ) is constant for all emotions, we can ignore it for classification purposes and focus on comparing ( P(X | E_i) . P(Ei) ) for each emotion.
A Naive Bayesian algorithm for recognizing emotions from text works on the principle of probabilistic classification. This assumes that the occurrence of each word in the text is independent of the presence of other words given the sense category. This "naive" assumption simplifies the calculation by considering the contribution of each word to the overall mood. In practice, the algorithm calculates the probability of each emotion based on the words in the text using Bayes' theorem. It calculates the prior probability of each emotion in the training data based on its occurrence, and the observation probability of each word based on the emotion. Multiplying these probabilities and comparing them between different emotional categories, the algorithm determines the emotional text with the highest probability. Despite its simplicity, Naive Bayes can be surprisingly effective at detecting sentiment from text, especially when trained on sufficiently large and diverse datasets.
Pseudo Code:
1. Data Preparation
  - Collect labeled text data.
   - Preprocess and split into training and testing sets.
2. Training:
   - Count word occurrences for each emotion.
   - Calculate prior probabilities for each emotion.
3. Classification:
   - Calculate likelihood of text for each emotion.
   - Multiply likelihood by prior probability.
   - Select emotion with highest probability.
4. Evaluation:
   - Compare predicted emotions with true labels.
   - Calculate evaluation metrics.
5. Improvement:
   - Experiment with preprocessing and tuning.
Bidirectional Encoder Representations from Transformers and Support Vector Machine :
Google created BERT, or Bidirectional Encoder Representations from Transformers, a state-of-the-art natural language processing (NLP) technique. The Transformer model, which is incorporated into its architecture, allows it to take context into account from both the left and right, completely changing the field of natural language processing. By using a technique known as masked language modeling, BERT predicts randomly masked words inside a phrase depending on the surrounding context, enabling bidirectionality. BERT is able to acquire rich contextual representations of words through this unsupervised pre-training, which may subsequently be optimized for a variety of downstream tasks including sentiment analysis or emotion identification. BERT uses a formula using multi-layer bidirectional Transformer encoders, with feed-forward neural networks and self-attention processes at each encoder layer. By using self-attention, BERT is able to assess the significance of various words.

	BERT is able to assess the relative value of words in a phrase using self-attention, and feed-forward networks enable non-linear transformations. When it comes to emotion recognition, BERT works by examining the text's contextual details to deduce the underlying emotions that are being expressed. BERT's ability to capture the finer points of language allows it to identify emotions, which is useful for a variety of applications, such as sentiment analysis on social media and customer feedback interpretation.While feed-forward networks enable non-linear transformations, BERT uses self-attention to assess the relative weights of various words in a sentence. In order to determine the underlying emotions expressed in a text, BERT analyzes the contextual information found in the text. BERT's ability to detect emotions in complex language is a useful tool for a variety of applications, such as sentiment analysis on social media and customer feedback interpretation.
Support Vector Machine:
A potent supervised machine learning approach for regression and classification problems is called Support Vector Machine (SVM). The fundamental idea behind it is to locate the appropriate hyperplane in a high-dimensional space to divide data points from various classes. Maximizing the interclass margin—the distance between the hyperplane and the nearest data points, or support vectors, for each class—is the aim of support vector machines (SVM). SVM increases generalization to unknown data in addition to achieving strong classification performance by optimizing this margin. SVM may analyze data that is linearly separable or non-linearly separable by employing a u of kernel functions, including sigmoid, polynomial, linear, and radial basis function (RBF). Because of its adaptability, SVM can effectively handle a variety of data sets. Because of their strength.
Detecting emotions from text using a Support Vector Machine (SVM)  algorithm involves several steps,u JJuu text preprocessing, feature m, training the SVM model, and then using the trained model to classify emotions in unseen text. Here's a basic outline of the process along with the formula for the SVM algorithm:

The formula for the SVM algorithm involves finding the hyperplane that maximizes the margin between classes while minimizing the classification error. Given a training dataset with feature vectors xi and correspoknding labels yi, where xi is a feature vector in a high-dimensiovnal space and yi is its class label, the SVM algorithm seeks to solve the following optimization problem:
subject to the consytraints:
                          yi(w.xi+b)>=1-(i)                                                                  (2)
                                                                i=1,.....,N and (i)>=0
Where:i uu
- w is the weight vector that defines the separating hyperplane.
- b is the bias term.
- C is the regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.
- xi are slack variables that allow for some misclassification.
-  N is the number of training examples.
Bv
Once the optimization problem is solved, the classification of new instances can be performed using the decision function.
                      f(x)=sign(w.x+b)                                                                                    (3) c 
where x is the feature vector of the new instance.
This decision function assigns a class label based on which side of the hyperplane the feature vector falls on. For example, if f(x)>0, the instance belongs to one class, and if  f(x)<0, it belongs to the other class.
Suitable for both regression and classification applications, Support Vector Machine (SVM) is a potent supervised machine learning technique.  Ou bIn a high-dimensional space, its fundamental idea is to choose the appropriate hyperplane for separating data points from various classes. Making the greatest interclass margin possible is the aim of support vector machines (SVM). This is the distance measured between the nearest support vectors, or data points, of each class and the hyperplane. SVM enhances both classification performance and generalization to new data by optimizing this margin. SVM uses a variety of kernel functions, including linear, polynomial, radial basis function (RBF), and sigmoid, to handle data that is both linearly and non-linearly separable. SVM can effectively handle a variety of data sets thanks to its adaptability. Owing to their resilience.
Pseudocode:
Step 1: 
   - Step 1.1: Split the dataset into training and testing sets.
   - Step 1.2: Convert text data into numerical feature vectors.
   - Step 1.2.1: Utilize techniques like TF-IDF to convert text into numerical representations.
   - Step 1.2.2: Ensure each text sample corresponds to a feature vector.
Step 2: 
   - Step 2.1: Initialize SVM classifier with desired parameters.
   - Step 2.1.1: Choose an appropriate kernel function (e.g., linear, RBF).
   - Step 2.1.2: Set hyperparameters like regularization parameter (C) and kernel coefficients.
Step 3: 
   - Step 3.1: Train the SVM classifier using the training data.
   - Step 3.1.1: Feed the feature vectors of the training set into the SVM model.
   - Step 3.1.2: SVM adjusts the hyperplane to maximize the margin between different classes.
Step 4: 
   - Step 4.1: Predict emotions on the test set.
   - Step 4.1.1: Utilize the trained SVM model to predict emotions for the test samples.
  - Step 4.1.2: Transform the text samples of the test set into feature vectors using the same method as in the training phase.
Step 5: 
   - Step 5.1: Evaluate the performance of the classifier using appropriate metrics.
   - Step 5.1.1: Compute metrics like precision, recall, F1-score, and accuracy.
   - Step 5.1.2: Compare predicted emotions with the actual emotions in the test set.
   - Step 5.1.3: Adjust parameters and repeat steps 3-5 if necessary for better performance.




Statistical Analysis:
The IBM SPSS program, version 25, was used to perform the statistical analysis for this study. It offered a graphical depiction of the F1 Score attained by the investigation by treating the brightness and contrast.The results of the Bidirectional Encoder Representations from Transformers and Support Vector Machine  and Novel Naive Bayes were compared using an one-way anova test.
Result:
The application of machine learning models to enhance the F1 Score of detecting the emotion from the text of a chosen dataset. The Novel Naive Bayes algorithm and Bidirectional Encoder Representations from Transformers and Support Vector Machine  Algorithm are examined, and detection is carried out successfully; the suggested study offers superior performance to the Naive Bayes algorithm.
Table 1 represents the F1 Score values for Novel Naive Bayes and Bidirectional Encoder Representations from Transformers and Support Vector Machine  algorithms based on that Novel Naive Bayes has shown high performance with the F1 Score value 93.925% 
Table 2 represents the sum of squares, df, mean squares, F and significance values for Novel Naive Bayes and Bidirectional Encoder Representations from Transformers and Support Vector Machine . Sum of squares value between the groups is 2075.870, within the groups is 5939.353 and the total sum of squares value is 8015.223
Figure 1 represents the bar graph for the Novel Naive Bayes and Bidirectional Encoder Representations from Transformers and Support Vector Machine  based on that we can conclude that Novel Naive Bayes has the higher performance and has  F1 Score of  93.925%.
Discussion:
The exploration of emotion detection from textual data has garnered significant attention, with a focus on leveraging advanced algorithms to enhance F1 Score. Various machine learning techniques including Naive Bayes and Bidirectional Encoder Representations from Transformers and Support Vector Machine s have been employed for this purpose.One of these, a new Naive Bayes algorithm, performed exceptionally well, outperforming the Bidirectional Encoder Representations from Transformers and Support Vector Machine , which had a F1 Score rate of 50% and 89.61% with an astounding F1 Score rate of 93.925%.The development expands upon the foundation works of Shivani Rana; Rakesh Kanji; Shruti Jain on Comparison of SVM and Naive Bayes for sentiment classification using BERT data (2022), remains a cornerstone in the field
However, despite the success in achieving high F1 Score, the current approach primarily focuses on emotion detection without considering the intricate nuances of human emotional expression and context. Just as the project detecting  the emotion from the text has limitations, this approach may overlook critical factors influencing emotional expression in text, such as temporal dynamics, contextual cues, and individual differences. To refine and broaden the applicability of the emotion detection model, a comprehensive framework incorporating a wider array of features, contextual information, and individual variability could offer a more nuanced understanding of text-based emotion detection, thereby enhancing its utility in diverse applications.
Conclusion :
In this research, Novel Naive Bayes compared to Bidirectional Encoder Representations from Transformers approach and Support Vector Machine  approach. Naive Bayes exhibited promising results by improving the F1 Score of the Text-Emotion Dataset. Table 1 displays the entire results. The highest F1 Score is 93.925% was achieved by employing Novel Naive bayes. These algorithms are therefore crucial for data analysis and detection. 
References:

                                                         Tables and Figures
Table 1 represents the F1 Score comparison of the conventional and proposed methods.



SI.NO
TEST SIZE
F1 Score(Naive Bayes)
F1 Score(Bidirectional Encoder Representations from Transformers)
F1 Score(Support Vector Machine)
1
TEST 1 - 4453
93.5
49.5
91.5
2
TEST 2 - 4453
94.1
51.2
88.25
3
TEST 3 - 4453
93.7
48.8
87.75
4
TEST 4 - 4453
93.8
52.1
90.00
5
TEST 5 - 4453
94.0
49.7
88.00
6
TEST 6 - 4453
93.9
47.3
90.75
7
TEST 7 - 4453
94.2
53.6
89.50
8
TEST 8 - 4453
93.6
48.4
89.00
9
TEST 9 - 4453
93.8
50.9
90.25
10
TEST 10 - 4453
94.3
50.3
89.75

Table 2 The sum of squares, df, mean square, F and significance of the existing and proposed methods were, respectively. In one-way anova analysis, the significance value is 0.17 which is statistically significant.
ANOVA
F1Score 
 
Sum of Squares
df
Mean Square
F
Sig.
Between Groups
2075.870
2
1037.935
4.718
.017
Within Groups
5939.353
27
219.976
 
 
Total
8015.223
29
 
 
 


Graph:

 Figure 1 represents the F1 Score and mean F1 Score calculation of three algorithms and the proposed over-selected input. The proposed method attained a mean F1 Score of 74.658
Declarations:
Conflicts of Interest:
 No conflict of interest in this manuscript. 
Author’s Contribution:
 
Author K.Harshith Sai was involved in data collection , data analysis and manuscript writing. Author Dr.D.Beulah David was involved in conceptualization, data validation and critical reviews of the manuscript.
Acknowledgements:
The authors would like to express their gratitude towards Saveetha School of Engineering, Saveetha Institute of Medical  and Technical Sciences (formerly known as Saveetha University) for providing the necessary infrastructure to carry out this work successfully.
Findings:
We Thank the following organizations for providing financial support that enabled us to complete this study. 
Saveetha School of Engineering.
Saveetha University.
Saveetha Institute of Medical and Technical Sciences.
References:
(Chen et al. 2020; Abhang, Gawali, and Mehrotra 2016; Grekow 2017; Poole and Willoughby 2024; Mishra 2022; Wedyan and Saeidi-Rizi 2024; Troiano 2023; Choi 2024; Ai et al. 2024; Matoušek and Mautner 2007; Silge and Robinson 2017; Ahmad 2011; Konar and Chakraborty 2015; Zhao et al. 2024)

Wedyan, Musab, and Fatemeh Saeidi-Rizi. 2024. “Assessing the Impact of Urban Environments on Mental Health and Perception Using Deep Learning: A Review and Text Mining Analysis.” Journal of Urban Health: Bulletin of the New York Academy of Medicine, March. https://doi.org/10.1007/s11524-024-00830-6.

Ahmad, Khurshid. 2011. Affective Computing and Sentiment Analysis: Emotion, Metaphor and Terminology. Springer Science & Business Media.

Ai, Wei, Yuntao Shou, Tao Meng, and Keqin Li. 2024. “DER-GCN: Dialog and Event Relation-Aware Graph Bidirectional Encoder Representations from Transformers and Support Vector Machine  for Multimodal Dialog Emotion Recognition.” IEEE Transactions on Neural Networks and Learning Systems PP (March). https://doi.org/10.1109/TNNLS.2024.3367940.

Chen, Luefeng, Min Wu, Witold Pedrycz, and Kaoru Hirota. 2020. Emotion Recognition and Understanding for Emotional Human-Robot Interaction Systems. Springer Nature.
Abhang, Priyanka A., Bharti W. Gawali, and Suresh C. Mehrotra. 2016. Introduction to EEG- and Speech-Based Emotion Recognition. Academic Press.


Grekow, Jacek. 2017. From Content-Based Music Emotion Recognition to Emotion Maps of Musical Pieces. Springer.

Konar, Amit, and Aruna Chakraborty. 2015. Emotion Recognition: A Pattern Analysis Approach. John Wiley & Sons.

Matoušek, Václav, and Pavel Mautner. 2007. Text, Speech and Dialogue: 10th International Conference, TSD 2007, Pilsen, Czech Republic, September 3-7, 2007, Proceedings. Springer Science & Business Media.

Mishra, Amit Kumar. 2022. Emotion Detection From Speech. BFC Publications.

Choi, Soyoung. 2024. “Perceived Challenges and Emotional Responses in the Daily Lives of Older Adults With Disabilities: A Text Mining Study.” Gerontology & Geriatric Medicine 10 (March): 23337214241237097.

Poole, Kristie L., and Teena Willoughby. 2024. “Children’s Shyness and Early Stages of Emotional Face Processing.” Biological Psychology, March, 108771.

Silge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. “O’Reilly Media, Inc.”

Troiano, Enrica. 2023. Where Are Emotions in Text? A Human-Based and Computational Investigation of Emotion Recognition and Generation.

Zhao, Jianbo, Huailiang Liu, Yakai Wang, Weili Zhang, Xiaojin Zhang, Bowei Li, Tong Sun, Yanwei Qi, and Shanzhuang Zhang. 2024. “Sentiment Analysis of Video Danmakus Based on MIBE-RoBERTa-FF-BiLSTM.” Scientific Reports 14 (1): 5827.


























